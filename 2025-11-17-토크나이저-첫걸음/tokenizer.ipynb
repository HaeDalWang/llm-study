{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì € ì²«ê±¸ìŒ ë…¸íŠ¸ë¶\n",
    "\n",
    "- ìˆœì„œëŒ€ë¡œ ë”°ë¼ê°€ë©´ **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ë„** ê¸°ë³¸ í† í°í™”ë¥¼ ì²´í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ë§ˆì§€ë§‰ì—ëŠ” `tokenizers`/`sentencepiece` ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì–´ë–»ê²Œ ì„¤ì¹˜í•´ ì“°ëŠ”ì§€ë„ ì•ˆë‚´í•©ë‹ˆë‹¤.\n",
    "- ì…€ë§ˆë‹¤ ì£¼ì„ìœ¼ë¡œ \"ë¬´ì—‡ì„ í•˜ë©´ ë˜ëŠ”ì§€\" ì ì–´ë‘ì—ˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ì‹¤í–‰ë§Œ í•´ë³´ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1a3831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['í•´ë¦¬í¬í„°ëŠ” ë§ˆë²•ì‚¬ì•¼!', 'AI ëª¨ë¸ì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ìš”.', 'ìƒˆë¡œìš´_í† í°?? ì•ˆë…•ğŸ™‚']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0. ì‹¤í—˜ ë¬¸ì¥ ì •ì˜ (ììœ ë¡­ê²Œ ìˆ˜ì • ê°€ëŠ¥)\n",
    "sentences = [\n",
    "    \"í•´ë¦¬í¬í„°ëŠ” ë§ˆë²•ì‚¬ì•¼!\",\n",
    "    \"AI ëª¨ë¸ì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ìš”.\",\n",
    "    \"ìƒˆë¡œìš´_í† í°?? ì•ˆë…•ğŸ™‚\"\n",
    "]\n",
    "\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c26641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì¥: í•´ë¦¬í¬í„°ëŠ” ë§ˆë²•ì‚¬ì•¼!\n",
      "- ê³µë°± ê¸°ì¤€: ['í•´ë¦¬í¬í„°ëŠ”', 'ë§ˆë²•ì‚¬ì•¼!']\n",
      "- ë¬¸ì ê¸°ì¤€(ì•ë¶€ë¶„): ['í•´', 'ë¦¬', 'í¬', 'í„°', 'ëŠ”', ' ', 'ë§ˆ', 'ë²•', 'ì‚¬', 'ì•¼', '!'] \n",
      "- êµ¬ë‘ì  ì¸ì§€: ['í•´ë¦¬í¬í„°ëŠ”', 'ë§ˆë²•ì‚¬ì•¼', '!']\n",
      "\n",
      "ë¬¸ì¥: AI ëª¨ë¸ì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ìš”.\n",
      "- ê³µë°± ê¸°ì¤€: ['AI', 'ëª¨ë¸ì€', 'ë‹¤ìŒ', 'ë‹¨ì–´ë¥¼', 'ì˜ˆì¸¡í•´ìš”.']\n",
      "- ë¬¸ì ê¸°ì¤€(ì•ë¶€ë¶„): ['A', 'I', ' ', 'ëª¨', 'ë¸', 'ì€', ' ', 'ë‹¤', 'ìŒ', ' ', 'ë‹¨', 'ì–´', 'ë¥¼', ' ', 'ì˜ˆ', 'ì¸¡', 'í•´', 'ìš”', '.'] \n",
      "- êµ¬ë‘ì  ì¸ì§€: ['AI', 'ëª¨ë¸ì€', 'ë‹¤ìŒ', 'ë‹¨ì–´ë¥¼', 'ì˜ˆì¸¡í•´ìš”', '.']\n",
      "\n",
      "ë¬¸ì¥: ìƒˆë¡œìš´_í† í°?? ì•ˆë…•ğŸ™‚\n",
      "- ê³µë°± ê¸°ì¤€: ['ìƒˆë¡œìš´_í† í°??', 'ì•ˆë…•ğŸ™‚']\n",
      "- ë¬¸ì ê¸°ì¤€(ì•ë¶€ë¶„): ['ìƒˆ', 'ë¡œ', 'ìš´', '_', 'í† ', 'í°', '?', '?', ' ', 'ì•ˆ', 'ë…•', 'ğŸ™‚'] \n",
      "- êµ¬ë‘ì  ì¸ì§€: ['ìƒˆë¡œìš´', '_', 'í† í°', '?', '?', 'ì•ˆë…•', 'ğŸ™‚']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def tokenize_whitespace(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenize_characters(text: str) -> List[str]:\n",
    "    return list(text)\n",
    "\n",
    "\n",
    "def tokenize_punct_aware(text: str) -> List[str]:\n",
    "    pattern = r\"[A-Za-z0-9_]+|[\\uAC00-\\uD7A3]+|[^\\sA-Za-z0-9_\\uAC00-\\uD7A3]\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "\n",
    "def show_all_basic(text: str) -> None:\n",
    "    print(f\"ë¬¸ì¥: {text}\")\n",
    "    ws = tokenize_whitespace(text)\n",
    "    print(\"- ê³µë°± ê¸°ì¤€:\", ws)\n",
    "    chars = tokenize_characters(text)\n",
    "    print(\"- ë¬¸ì ê¸°ì¤€(ì•ë¶€ë¶„):\", chars[:40], (\"...\" if len(chars) > 40 else \"\"))\n",
    "    punct = tokenize_punct_aware(text)\n",
    "    print(\"- êµ¬ë‘ì  ì¸ì§€:\", punct)\n",
    "    print()\n",
    "\n",
    "for s in sentences:\n",
    "    show_all_basic(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76239454",
   "metadata": {},
   "source": [
    "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì“°ë©´ ë­ê°€ ì¢‹ì„ê¹Œ?\n",
    "- `tokenizers` (Hugging Face)ë‚˜ `sentencepiece` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì“°ë©´ **BPE, WordPiece, Unigram** ê°™ì€ ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜ì„ ì‰½ê²Œ ì²´í—˜í•  ìˆ˜ ìˆì–´ìš”.\n",
    "- ì„¤ì¹˜ ëª…ë ¹ ì˜ˆì‹œ (ê°€ìƒí™˜ê²½ì—ì„œ ì‹¤í–‰ ê¶Œì¥):\n",
    "  ```bash\n",
    "  pip install tokenizers sentencepiece\n",
    "  ```\n",
    "- ì•„ë˜ ì…€ì€ `tokenizers`ê°€ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ëŠ” ê°€ì • í•˜ì— ëŒì•„ê°‘ë‹ˆë‹¤. ì—†ìœ¼ë©´ ë¨¼ì € ìœ„ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bb320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "=== BPE vocab ì¼ë¶€ ===\n",
      "['P', '[UNK]', 'ë¡œ', 'ì‚¬', 't', 'ë°”', 'ì´ë¡œ', 'c', 'ë“ ', 'B', 'ë§ˆë²•', 'ë§Œë“ ë‹¤', 'ë¦¬', 'ì§€', 'ë¸ì€', 'ì´', 'ì™¸ìš´ë‹¤', 'ìˆ«', 'ë‹¤', 'ì§€íŒ¡ì´ë¡œ']\n",
      "\n",
      "=== ìƒ˜í”Œ ë¬¸ì¥ í† í¬ë‚˜ì´ì¦ˆ ===\n",
      "í•´ë¦¬í¬í„°ëŠ” ë§ˆë²•ì‚¬ì•¼!\n",
      "í† í°: ['í•´ë¦¬í¬í„°ëŠ”', 'ë§ˆë²•ì‚¬ì•¼', '!']\n",
      "í† í° ID: [94, 86, 4]\n",
      "\n",
      "AI ëª¨ë¸ì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ìš”.\n",
      "í† í°: ['AI', 'ëª¨ë¸ì€', 'ë‹¤', '[UNK]', '[UNK]', '[UNK]', 'ë¥¼', '[UNK]', '[UNK]', 'í•´', '[UNK]', '.']\n",
      "í† í° ID: [57, 70, 19, 1, 1, 1, 24, 1, 1, 52, 1, 5]\n",
      "\n",
      "ìƒˆë¡œìš´_í† í°?? ì•ˆë…•ğŸ™‚\n",
      "í† í°: ['[UNK]', 'ë¡œ', 'ìš´', '[UNK]', 'í† ', 'í°', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n",
      "í† í° ID: [1, 23, 39, 1, 49, 47, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ„ì˜ pip ëª…ë ¹ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\") from e\n",
    "\n",
    "# Step 1. ê°„ë‹¨í•œ ë§ë­‰ì¹˜ ì¤€ë¹„ (ì‹¤ì œë¡  ë” í¬ê³  ë‹¤ì–‘í•´ì•¼ í•¨)\n",
    "corpus = [\n",
    "    \"í•´ë¦¬í¬í„°ëŠ” ë§ˆë²•ì‚¬ì•¼!\",\n",
    "    \"ë§ˆë²•ì‚¬ëŠ” ì§€íŒ¡ì´ë¡œ ì£¼ë¬¸ì„ ì™¸ìš´ë‹¤.\",\n",
    "    \"AI ëª¨ë¸ì€ í† í°ì„ ìˆ«ìë¡œ ë°”ê¾¼ë‹¤.\",\n",
    "    \"SentencePieceì™€ BPEëŠ” ì„œë¸Œì›Œë“œë¥¼ ë§Œë“ ë‹¤.\",\n",
    "]\n",
    "\n",
    "# Step 2. BPE ëª¨ë¸ ìƒì„±\n",
    "bpe_model = models.BPE(unk_token=\"[UNK]\")\n",
    "trainer = trainers.BpeTrainer(vocab_size=100, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"])\n",
    "tokenizer = Tokenizer(bpe_model)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "print(\"=== BPE vocab ì¼ë¶€ ===\")\n",
    "print(list(tokenizer.get_vocab().keys())[:20])\n",
    "\n",
    "print(\"\\n=== ìƒ˜í”Œ ë¬¸ì¥ í† í¬ë‚˜ì´ì¦ˆ ===\")\n",
    "for sentence in sentences:\n",
    "    output = tokenizer.encode(sentence)\n",
    "    print(sentence)\n",
    "    print(\"í† í°:\", output.tokens)\n",
    "    print(\"í† í° ID:\", output.ids)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217fe46",
   "metadata": {},
   "source": [
    "### ì •ë¦¬ ë©”ëª¨\n",
    "- **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´**ë„ ê³µë°±/ë¬¸ì/êµ¬ë‘ì  ë‹¨ìœ„ë¡œ ìª¼ê°œë³´ë©° ê°ì„ ì¡ì„ ìˆ˜ ìˆë‹¤.\n",
    "- **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©** ì‹œì—ëŠ” BPE ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ \"ìì£¼ ë‚˜ì˜¤ëŠ” ì¡°ê°\"ì„ ìë™ìœ¼ë¡œ vocabì— ë„£ì–´ ì¤€ë‹¤.\n",
    "- vocab í¬ê¸°ë‚˜ ë§ë­‰ì¹˜ë¥¼ ë°”ê¾¸ë©´ í† í° ìˆ˜ì™€ ì¡°ê°ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ì§ì ‘ ì‹¤í—˜í•´ë³´ì.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
