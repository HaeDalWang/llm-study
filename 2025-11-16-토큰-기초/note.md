# 오늘의 기록

- 날짜: 2025-11-16
- 주제: 토큰이 뭐지?
- 내가 이해한 한 줄 정리: 토큰은 문장을 모델이 다루기 쉽게 잘게 나눈 단위다.
- 새로 배운 용어:
  - 토큰(Token): 단어/부분 단어 같은 최소 단위
  - 어휘집(Vocabulary): 모델이 아는 토큰들의 목록
- 다음에 궁금한 것:
  - 토크나이저는 어떤 방식으로 문장을 쪼개지?
  - BPE와 SentencePiece는 무엇이 다를까?

## 아주 짧은 예시
- 문장: "해리포터는 마법사야!"
- 토큰화 예(가능한 형태 중 하나): ["해리포터", "는", "마법사", "야", "!"]
  - 실제 결과는 사용하는 토크나이저에 따라 달라질 수 있음

## 왜 굳이 토큰으로 바꾸나? (아주 쉽게)
- 컴퓨터/모델은 “글자 자체”를 이해하지 못함 → 먼저 “작은 조각(토큰)”으로 바꿔서 숫자로 다룸
- 너무 큰 덩어리(문장 통째)로 다루면 복잡·느림, 너무 작은 덩어리(한 글자)만 쓰면 의미가 약해짐  
  → 적당한 크기의 조각(토큰)으로 쪼개면 “속도/의미” 균형이 좋아짐
- 새 단어(신조어, 오타)가 나와도 조각들을 이어서 표현 가능(완전히 모르는 단어를 최소화)

## vocab(어휘집)이 뭐고, 내가 만들 필요 있어?
- vocab = “모델이 아는 토큰 목록(사전)”  
- 보통 “내가 직접 만들 필요 없음”  
  - 공개 모델은 “그 모델 전용 토크나이저+vocab”이 이미 있음  
  - 같은 모델을 쓸 때는 그 토크나이저를 그대로 써야 결과가 맞음(서로 호환)
- 직접 만들 때는?  
  - 특수 분야(의료/법률 등) 용어가 많을 때  
  - 내 모델을 처음부터(또는 크게 바꿔) 훈련할 때

## “벡터/임베딩/인덱스”를 아주 쉬운 말로
- 벡터 = 여러 개 숫자를 줄 세운 것(예: [0.1, 0.8, -0.2])  
  → “단어/토큰의 느낌”을 숫자로 표현한 카드라고 생각
- 임베딩 = 그 “느낌 카드(벡터)”를 만드는 방법/표현  
- 인덱스 = “사전 번호표” 같은 것. 토큰마다 번호가 있어야 찾을 수 있음

## 메모
- 토큰 단위가 달라지면 길이, 비용, 모델의 이해 방식에도 영향이 생긴다.
- 다음에는 토크나이저 종류(BPE, WordPiece, SentencePiece) 차이를 가볍게 비교해보자.


