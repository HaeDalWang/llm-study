{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 첫걸음 노트북\n",
    "\n",
    "아래 순서를 그대로 따라가면 “토크나이저가 무엇을 하는지”를 한 단계씩 체험할 수 있습니다.\n",
    "\n",
    "- Step 0: 오늘 실험에 쓸 문장을 고른다\n",
    "- Step 1: 공백 기준으로 나눠 본다 → 왜 필요한지 적어 본다\n",
    "- Step 2: 문자 기준으로 나눠 본다 → 길이 차이 관찰\n",
    "- Step 3: 구두점/기호까지 챙긴 간단한 규칙 적용\n",
    "- Step 4: (선택) 지금까지 결과를 비교하며 메모\n",
    "- Step 5: 라이브러리를 이용해 BPE 토크나이저를 직접 만들어 본다\n",
    "\n",
    "각 Step은 “설명 → 함수 정의 → 실행 결과” 순서로 분리돼 있습니다. 필요한 셀만 실행하고 다음 단계로 넘어가세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e1a3831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['저의 이름 배승도에요.', '전화 번호는 010-1234-5678이에요.', 'Dobby is Free!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 0. 실험 문장 정의 (자유롭게 수정 가능)\n",
    "sentences = [\n",
    "    \"저의 이름 배승도에요.\",\n",
    "    \"전화 번호는 010-1234-5678이에요.\",\n",
    "    \"Dobby is Free!\"\n",
    "]\n",
    "# 참고로 sentences는 문장이라는 뜻이다 영어로\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26641c",
   "metadata": {},
   "source": [
    "## Step 1 · 공백 기준 토큰화\n",
    "- 목표: 파이썬의 `split()`으로 문장을 “단어 단위”로 쉽게 나눌 수 있음을 확인\n",
    "- 진행 순서\n",
    "  1. 함수를 정의한다.\n",
    "  2. 방금 정의한 함수로 `sentences`의 각 문장을 나눠 본다.\n",
    "- 실행 후에는 “공백만 믿으면 안 되는 경우가 뭘까?”를 스스로 메모해 보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cce89f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "함수가 준비되었습니다. 이제 아래 셀에서 사용해 보세요.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_whitespace(text: str):\n",
    "    \"\"\"공백(스페이스) 기준으로 문장을 나눕니다.\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "print(\"함수가 준비되었습니다. 이제 아래 셀에서 사용해 보세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2f2644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 저의 이름 배승도에요.\n",
      "→ 공백 기준 토큰: ['저의', '이름', '배승도에요.']\n",
      "\n",
      "문장: 전화 번호는 010-1234-5678이에요.\n",
      "→ 공백 기준 토큰: ['전화', '번호는', '010-1234-5678이에요.']\n",
      "\n",
      "문장: Dobby is Free!\n",
      "→ 공백 기준 토큰: ['Dobby', 'is', 'Free!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    tokens = tokenize_whitespace(sentence)\n",
    "    print(f\"문장: {sentence}\")\n",
    "    print(\"→ 공백 기준 토큰:\", tokens)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e4768",
   "metadata": {},
   "source": [
    "## Step 2 · 문자(글자) 기준 토큰화\n",
    "- 목표: 한 글자씩 쪼개면 토큰 수가 급격히 늘어남을 확인\n",
    "- 진행 순서\n",
    "  1. 함수 정의\n",
    "  2. 문장마다 앞부분 40글자 정도만 미리보기\n",
    "- 실행 후 “글자 단위만 쓰면 어떤 문제가 생길까?”를 적어두세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f643e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 단위 함수 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "def tokenize_characters(text: str):\n",
    "    \"\"\"문장을 한 글자씩 리스트로 만듭니다.\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "print(\"문자 단위 함수 준비 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1618ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 저의 이름 배승도에요.\n",
      "→ 문자 기준 토큰 수: 12\n",
      "→ 앞부분 미리보기: ['저', '의', ' ', '이', '름', ' ', '배', '승', '도', '에', '요', '.'] \n",
      "\n",
      "문장: 전화 번호는 010-1234-5678이에요.\n",
      "→ 문자 기준 토큰 수: 24\n",
      "→ 앞부분 미리보기: ['전', '화', ' ', '번', '호', '는', ' ', '0', '1', '0', '-', '1', '2', '3', '4', '-', '5', '6', '7', '8', '이', '에', '요', '.'] \n",
      "\n",
      "문장: Dobby is Free!\n",
      "→ 문자 기준 토큰 수: 14\n",
      "→ 앞부분 미리보기: ['D', 'o', 'b', 'b', 'y', ' ', 'i', 's', ' ', 'F', 'r', 'e', 'e', '!'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    chars = tokenize_characters(sentence)\n",
    "    preview = chars[:40]\n",
    "    print(f\"문장: {sentence}\")\n",
    "    print(\"→ 문자 기준 토큰 수:\", len(chars))\n",
    "    print(\"→ 앞부분 미리보기:\", preview, (\"...\" if len(chars) > 40 else \"\"))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58f3d5",
   "metadata": {},
   "source": [
    "## Step 3 · 구두점/기호까지 챙기기\n",
    "- 목표: 단어/숫자/한글 블록과 구두점을 분리하는 간단한 정규식 사용해 보기\n",
    "- 진행 순서\n",
    "  1. 정규식 기반 함수 정의\n",
    "  2. 문장마다 결과 확인\n",
    "- 실행 후 “어떤 기호가 아직 잘 안 나눠지는가?”를 체크하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7871f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구두점 인지 함수 준비!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize_punct_aware(text: str):\n",
    "    \"\"\"단어/숫자/한글 블록과 구두점을 분리합니다.\"\"\"\n",
    "    # 해당 정규표현식은 대략 문자, 숫자, 한글 블록을 분리합니다.\n",
    "    pattern = r\"[A-Za-z0-9_]+|[\\uAC00-\\uD7A3]+|[^\\sA-Za-z0-9_\\uAC00-\\uD7A3]\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "print(\"구두점 인지 함수 준비!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e07354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 저의 이름 배승도에요.\n",
      "→ 구두점 인지 토큰: ['저의', '이름', '배승도에요', '.']\n",
      "\n",
      "문장: 전화 번호는 010-1234-5678이에요.\n",
      "→ 구두점 인지 토큰: ['전화', '번호는', '010', '-', '1234', '-', '5678', '이에요', '.']\n",
      "\n",
      "문장: Dobby is Free!\n",
      "→ 구두점 인지 토큰: ['Dobby', 'is', 'Free', '!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    tokens = tokenize_punct_aware(sentence)\n",
    "    print(f\"문장: {sentence}\")\n",
    "    print(\"→ 구두점 인지 토큰:\", tokens)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2dd20",
   "metadata": {},
   "source": [
    "## Step 4 · 짧은 비교 메모\n",
    "- 세 방식의 차이를 직접 적어보세요. (예: \"공백 방식은 '?'와 붙어 있음\", \"문자 방식은 길이가 길다\")\n",
    "- 필요하면 아래 빈 셀에 표 혹은 Bullet로 정리하고, 완료 후 이 셀 아래에 메모를 남겨도 좋습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76239454",
   "metadata": {},
   "source": [
    "## Step 5 · 라이브러리(BPE) 맛보기\n",
    "- 이제 실제 토크나이저 라이브러리가 어떻게 “자주 나오는 조각”을 vocab에 넣는지 체험합니다.\n",
    "- 필요한 준비\n",
    "  1. `pip install tokenizers sentencepiece` (이미 설치되어 있으면 건너뛰기)\n",
    "  2. 커널은 `.venv` 혹은 `Python (llm-study)`처럼 패키지가 설치된 환경으로 선택\n",
    "- 아래 셀들은 **Import → 학습 데이터 준비 → 학습/결과 확인** 순서로 나뉘어 있습니다. 셀을 하나씩 실행하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86bb320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers 모듈 임포트 성공!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "    print(\"tokenizers 모듈 임포트 성공!\")\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"tokenizers 라이브러리가 설치되어 있지 않습니다. 위의 pip 명령을 먼저 실행하세요.\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41964455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "학습 완료! vocab 크기: 75\n"
     ]
    }
   ],
   "source": [
    "# BPE는 비트 프리픽스 인코딩이라는 뜻이다\n",
    "\n",
    "# corpus는 말뭉치라는 뜻이다\n",
    "corpus = [\n",
    "    \"저의 이름은 배승도 에요!\",\n",
    "    \"Dobby는 자유에요!\",\n",
    "    \"도비 is free!\",\n",
    "    \"전화번호는 010-1234-5678이에요.\"\n",
    "]\n",
    "\n",
    "# unk_token은 unknown token의 약자이다\n",
    "bpe_model = models.BPE(unk_token=\"[UNK]\")\n",
    "# trainer는 학습을 돕는 도구이다\n",
    "trainer = trainers.BpeTrainer(\n",
    "    # vocab_size는 어휘 크기이다, 즉 토큰의 종류 수이다\n",
    "    vocab_size=1000,\n",
    "    # special_tokens는 특수 토큰이다, 즉 특별한 의미를 가지는 토큰이다\n",
    "    # 각각 패딩, 미지정, 클래스, 구분 토큰이다\n",
    "    # 패딩 토큰은 문장의 길이를 맞추기 위해 사용되는 토큰이다\n",
    "    # 미지정 토큰은 모델이 모르는 토큰이다\n",
    "    # 클래스 토큰은 클래스 분류 모델에서 사용되는 토큰이다 > 예를 들어 문장이 긍정인지 부정인지 분류하는 모델에서 사용되는 토큰이다\n",
    "    # 구분 토큰은 문장을 구분하는 토큰이다\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    ")\n",
    "tokenizer = Tokenizer(bpe_model)\n",
    "# pre_tokenizer는 토큰화 전에 문장을 전처리하는 도구이다 > 예를 들어 문장을 공백 기준으로 나누는 도구이다 위에서 사용한 tokenize_whitespace 함수가 이 역할을 한다\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "# train_from_iterator는 학습을 시작한다\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "print(\"학습 완료! vocab 크기:\", len(tokenizer.get_vocab()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be588d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BPE vocab 일부 ===\n",
      "['r', '5', '6', '전화번호는', '유에요', '는', '저의', '화', 'i', '-', '[CLS]', '3', 'free', 'e', '이름은', '이에요', 'b', '8', '78', '배승']\n",
      "\n",
      "=== 샘플 문장 토크나이즈 ===\n",
      "저의 이름 배승도에요.\n",
      "→ 토큰: ['저의', '이', '름', '배승도', '에요', '.']\n",
      "→ 토큰 ID: [63, 37, 27, 70, 43, 6]\n",
      "\n",
      "전화 번호는 010-1234-5678이에요.\n",
      "→ 토큰: ['전화', '번호는', '010', '-', '1234', '-', '5678이에요', '.']\n",
      "→ 토큰 ID: [64, 71, 65, 5, 66, 5, 73, 6]\n",
      "\n",
      "Dobby is Free!\n",
      "→ 토큰: ['Dobb', 'y', 'is', '[UNK]', 'r', 'ee', '!']\n",
      "→ 토큰 ID: [68, 24, 53, 1, 22, 51, 4]\n",
      "\n",
      "[UNK]가 많다면? 말뭉치/어휘 크기를 늘려 다시 학습해 보세요.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BPE vocab 일부 ===\")\n",
    "print(list(tokenizer.get_vocab().keys())[:20])\n",
    "\n",
    "print(\"\\n=== 샘플 문장 토크나이즈 ===\")\n",
    "for sentence in sentences:\n",
    "    encoded = tokenizer.encode(sentence)\n",
    "    print(sentence)\n",
    "    print(\"→ 토큰:\", encoded.tokens)\n",
    "    print(\"→ 토큰 ID:\", encoded.ids)\n",
    "    print()\n",
    "\n",
    "print(\"[UNK]가 많다면? 말뭉치/어휘 크기를 늘려 다시 학습해 보세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217fe46",
   "metadata": {},
   "source": [
    "### Step 총정리\n",
    "- Step 1~3: 파이썬 기본기만으로도 토큰 단위를 직접 확인할 수 있다.\n",
    "- Step 4: 차이를 글로 적어둬야 나중에 다시 봐도 이해가 쉽다.\n",
    "- Step 5: 직접 BPE 토크나이저를 학습하면 말뭉치/어휘 크기에 따라 `[UNK]` 비율이 달라진다.\n",
    "- Step 6: 많이 쓰이는 공개 토크나이저(예: BERT)를 그대로 불러 쓰면 범용성이 높고, 다른 사람들과 동일한 토큰 ID를 공유할 수 있다.\n",
    "\n",
    "직접 트레이닝을 통해 토크나이저를 만들어야되는 경우는 특수 용어 의학용어등 일반적이지 않은 경우 기존 토크나이저들에 합쳐서 추가 트레이닝을 하는 경우에 위와같이 사용된다\n",
    "일반적인 모델 학습에 경우 이미 남들이 잘 만들어둔 토크나이저를 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd790fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9978c82f",
   "metadata": {},
   "source": [
    "## Step 6 · 공개 모델 토크나이저 불러오기\n",
    "- 2025년 현재 가장 널리 쓰이는 토크나이저 중 하나는 **BERT 계열**입니다.\n",
    "- 여기서는 `bert-base-multilingual-cased` 토크나이저를 불러옵니다.\n",
    "- 준비 사항\n",
    "  1. 인터넷 연결 (사전 파일을 처음 한 번 다운로드)\n",
    "  2. `pip install transformers` 또는 `uv add transformers`\n",
    "- 아래 셀들은 **Import → 토크나이저 불러오기 → 문장 토큰화** 순서입니다. Step 0에서 정의한 `sentences`를 그대로 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "050e5bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 모듈 임포트 성공! 버전: 4.57.1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    import transformers\n",
    "    print(\"transformers 모듈 임포트 성공! 버전:\", transformers.__version__)\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\"'transformers' 패키지가 필요합니다. `pip install transformers` 후 다시 실행하세요.\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "470487d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217c05d934994492827112f7e701ed56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043253148cef44c38ee989f08b680978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476c40cb5bd94fbfbef45c49c1a3f735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5603860e36734f73ae9264a312a262db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 'bert-base-multilingual-cased' 불러오기 완료!\n",
      "vocab 크기: 119547\n",
      "특수 토큰: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"토크나이저 '{model_name}' 불러오기 완료!\")\n",
    "print(\"vocab 크기:\", len(hf_tokenizer.get_vocab()))\n",
    "print(\"특수 토큰:\", hf_tokenizer.special_tokens_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efed2be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저의 이름 배승도에요.\n",
      "→ 토큰: ['[CLS]', '저', '##의', '이', '##름', '배', '##승', '##도에', '##요', '.', '[SEP]']\n",
      "→ 토큰 ID: [101, 9663, 10459, 9638, 49543, 9330, 48210, 108521, 48549, 119, 102]\n",
      "\n",
      "전화 번호는 010-1234-5678이에요.\n",
      "→ 토큰: ['[CLS]', '전', '##화', '번', '##호는', '010', '-', '1234', '-', '567', '##8', '##이', '##에', '##요', '.', '[SEP]']\n",
      "→ 토큰 ID: [101, 9665, 18227, 9338, 100543, 49470, 118, 82749, 118, 52603, 11396, 10739, 10530, 48549, 119, 102]\n",
      "\n",
      "Dobby is Free!\n",
      "→ 토큰: ['[CLS]', 'Do', '##bby', 'is', 'Free', '!', '[SEP]']\n",
      "→ 토큰 ID: [101, 11791, 31444, 10124, 16122, 106, 102]\n",
      "\n",
      "Tip) WordPiece 토크나이저라 '##'가 붙으면 앞 토큰에 이어지는 조각입니다.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    encoded = hf_tokenizer(sentence, add_special_tokens=True)\n",
    "    token_ids = encoded[\"input_ids\"]\n",
    "    tokens = hf_tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    print(sentence)\n",
    "    print(\"→ 토큰:\", tokens)\n",
    "    print(\"→ 토큰 ID:\", token_ids)\n",
    "    print()\n",
    "\n",
    "print(\"Tip) WordPiece 토크나이저라 '##'가 붙으면 앞 토큰에 이어지는 조각입니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
